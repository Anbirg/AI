{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "3AoxBbiWkhhJ",
        "outputId": "7f092dc4-8ad4-42b5-b1c3-b99bb69b9239"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3590198812.py:89: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9a0e7ea9e1b131316f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9a0e7ea9e1b131316f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "!pip install sentence-transformers pandas openpyxl fuzzywuzzy gradio --quiet\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/FAQ_1.xlsx\"\n",
        "\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "# FAQ\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    df = df.rename(columns={'Питання': 'question', 'Відповідь': 'answer'})\n",
        "else:\n",
        "    df = pd.DataFrame(columns=[\"question\", \"answer\"])\n",
        "    df.to_excel(file_path, index=False)\n",
        "\n",
        "\n",
        "# NLP\n",
        "\n",
        "# Model for creating embeddings (Russian/Ukrainian support)\n",
        "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
        "\n",
        "# Update embeddings\n",
        "def update_embeddings():\n",
        "    global df, embeddings\n",
        "    if len(df) > 0:\n",
        "        embeddings = model.encode(df['question'].tolist(), convert_to_tensor=True)\n",
        "    else:\n",
        "        embeddings = None\n",
        "\n",
        "update_embeddings()\n",
        "\n",
        "\n",
        "# Chatbot functions\n",
        "\n",
        "def get_answer(user_question):\n",
        "    user_question = user_question.strip()\n",
        "\n",
        "\n",
        "    greetings = [\"привет\", \"добрый день\", \"доброго дня\", \"добрий день\", \"здрастуйте\", \"хай\", \"привіт\"]\n",
        "    farewells = [\"пока\", \"до свидания\", \"до зустрічі\", \"всього доброго\", \"bye\", \"до побачення\"]\n",
        "\n",
        "    if any(g in user_question.lower() for g in greetings):\n",
        "        return \"Привіт! Чим я можу вам допомогти? \"\n",
        "    if any(f in user_question.lower() for f in farewells):\n",
        "        return \"До зустрічі!\"\n",
        "\n",
        "\n",
        "    if len(df) == 0:\n",
        "        return \"База пуста :(\"\n",
        "\n",
        "# Creating a question embedding\n",
        "    question_embedding = model.encode([user_question], convert_to_tensor=True)\n",
        "\n",
        "    # Cos\n",
        "    cos_sim = torch.nn.functional.cosine_similarity(question_embedding, embeddings)\n",
        "    best_idx = torch.argmax(cos_sim).item()\n",
        "\n",
        "    if cos_sim[best_idx] > 0.6:  # similarity threshold\n",
        "        return df.iloc[best_idx]['answer']\n",
        "    else:\n",
        "        return \"Я поки що не знаю відповіді. Додайте питання та відповдь, щоб я став краще!\"\n",
        "\n",
        "def add_qa(new_question, new_answer):\n",
        "    global df\n",
        "    new_question = new_question.strip()\n",
        "    new_answer = new_answer.strip()\n",
        "    if new_question and new_answer:\n",
        "        df = pd.concat([df, pd.DataFrame([[new_question, new_answer]], columns=['question', 'answer'])], ignore_index=True)\n",
        "        df.to_excel(file_path, index=False)\n",
        "        update_embeddings()\n",
        "        return \"Питання та відповідь доданні до бази!\"\n",
        "    else:\n",
        "        return \"Заповніть обидва поля, будь ласка\"\n",
        "\n",
        "\n",
        "# Gradio\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Банківский чат-бот (RU/UA)\")\n",
        "\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Ваше питання\")\n",
        "    with gr.Row():\n",
        "        submit = gr.Button(\"Надіслати\")\n",
        "\n",
        "    with gr.Accordion(\"Додати нове запитання\", open=False):\n",
        "        new_question = gr.Textbox(label=\"Нове запитання\")\n",
        "        new_answer = gr.Textbox(label=\"Нова відповідь\")\n",
        "        add_btn = gr.Button(\"Додати до бази\")\n",
        "        add_output = gr.Textbox(label=\"Статус\")\n",
        "\n",
        "\n",
        "    def respond(message, chat_history):\n",
        "        answer = get_answer(message)\n",
        "        chat_history.append((message, answer))\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    submit.click(respond, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
        "    add_btn.click(add_qa, inputs=[new_question, new_answer], outputs=[add_output])\n",
        "\n",
        "demo.launch()\n"
      ]
    }
  ]
}